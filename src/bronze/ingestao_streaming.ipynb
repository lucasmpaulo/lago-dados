{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d3d9b453-d4c5-46dc-980c-14b77538f451",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instalando Biblioteca"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f82a92-9840-438d-9166-4e9bee9c7299",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports biblioteca"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def table_exists(database, table):\n",
    "    count = (spark.sql(f\"SHOW TABLES FROM {database}\")\n",
    "                  .filter(f\"database='{database}' AND tableName='{table}'\")\n",
    "                  .count())    \n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1d4814-2010-401b-ad51-285118094bd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup: Parametrização"
    }
   },
   "outputs": [],
   "source": [
    "database = \"bronze\"\n",
    "tablename = \"customers\"\n",
    "id_field = \"idCliente\"\n",
    "timestamp_field = \"DtAtualizacao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48f3a4-cc29-489a-bf0f-f0463b07f6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O arquivo CSV não possui schea, então foi passado alguns parâmetros para ele definir. \n",
    "# Já com arquivos Parquet,  naturalmente já vem o schema inferido pois possui metadados.\n",
    "df_full = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/{tablename}/\")\n",
    "schema = df_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5163371-3805-44e8-9245-ce253dd64d47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão Carga Completa"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(database, tablename):\n",
    "    print(\"Tabela não existente, criando tabela...\")\n",
    "    df_full = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/{tablename}/\")\n",
    "    (df_full.coalesce(1).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema}.{tablename}\"))\n",
    "else:\n",
    "    print(\"Tabela já existente, ignorando a carga completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7490be8d-4a0a-4591-9bcb-f5b3d772bc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Atualização da tabela - ReadStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a943846-25bb-46f1-85ee-ea7978b403a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Carga CDC Do Kaggle"
    }
   },
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"teocalvo/teomewhy-loyalty-system\")\n",
    "print(f\"Dataset baixado em: {path}\")\n",
    "\n",
    "# Definir destino dos arquivos\n",
    "dest_path =  \"/Volumes/workspace/upsell/full_load/kaggle\"\n",
    "os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Copiando arquivos para o diretório de destino\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(\"clientes\") and file.endswith(\".csv\"):\n",
    "        shutil.copy(os.path.join(path, file), dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68d0475-33ed-4447-9ce4-5a3eb0c2c410",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771898271115}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_teste = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/kaggle\")\n",
    "\n",
    "display(df_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4450f4-c012-48e3-a387-84f34a20f859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ler os arquivos no formato Streaming"
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{schema}.{tablename}\")\n",
    "\n",
    "def upsert(df, deltatable):\n",
    "  df.createOrReplaceGlobalTempView(f\"view_{tablename}\")\n",
    "\n",
    "  query = f'''\n",
    "      SELECT * \n",
    "      FROM global_temp.view_{tablename}\n",
    "      QUALIFY ROW_NUMBER() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "  '''\n",
    "\n",
    "  df_cdc = spark.sql(query)\n",
    "\n",
    "  (deltatable.alias(\"b\")\n",
    "             .merge(df_cdc.alias(\"d\"), f\"b.{id_field} = d.{id_field}\")\n",
    "             .whenMatchedDelete(condition = \"d.OP = 'D'\")\n",
    "             .whenMatchedUpdateAll(condition = \"d.OP = 'U'\")\n",
    "             .whenNotMatchedInsertAll(condition = \"d.OP = 'I' or d.OP = 'U'\")\n",
    "             .execute()\n",
    "  )\n",
    "\n",
    "# Dataframe que realiza a leitura dos dados no formato stream.\n",
    "df_stream = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\n",
    "  #.option(\"cloudFiles.maxFilesPerTrigger\", 500)\n",
    "  .schema(schema)\n",
    "  .load(f\"/Volumes/workspace/upsell/cdc/{tablename}/\"))\n",
    "\n",
    "# Etapa que realiza a persistência dos dados.\n",
    "# Para cada batch recebido, ele aplicará uma função chamada upsert que recebe o pedaço de dados e a base onde será salvo.\n",
    "stream = (df_stream.writeStream\n",
    "          .option(\"checkpointLocation\", f\"/Volumes/workspace/upsell/cdc/{tablename}_checkpoint/\")\n",
    "          .foreachBatch(lambda df, batchID: upsert(df, bronze))\n",
    "          .trigger(availableNow=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c83c4ae-151d-4a85-b20e-7c6c90c8ab3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca8e959-ec22-421a-b8d9-b9d85ac9cc7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM bronze.customers"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
