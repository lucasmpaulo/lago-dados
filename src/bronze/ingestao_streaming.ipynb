{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d3d9b453-d4c5-46dc-980c-14b77538f451",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instalando Biblioteca"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f82a92-9840-438d-9166-4e9bee9c7299",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports biblioteca"
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub, shutil, os, delta, time\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def table_exists(database, table):\n",
    "    count = (spark.sql(f\"SHOW TABLES FROM {database}\")\n",
    "                  .filter(f\"database='{database}' AND tableName='{table}'\")\n",
    "                  .count())    \n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1d4814-2010-401b-ad51-285118094bd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup: Parametrização"
    }
   },
   "outputs": [],
   "source": [
    "database = \"bronze\"\n",
    "tablename = \"customers\"\n",
    "id_field = \"idCliente\"\n",
    "timestamp_field = \"DtAtualizacao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48f3a4-cc29-489a-bf0f-f0463b07f6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O arquivo CSV não possui schea, então foi passado alguns parâmetros para ele definir. \n",
    "# Já com arquivos Parquet,  naturalmente já vem o schema inferido pois possui metadados.\n",
    "df_full = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/{tablename}/\")\n",
    "schema = df_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5163371-3805-44e8-9245-ce253dd64d47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão Carga Completa"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(database, tablename):\n",
    "    print(\"Tabela não existente, criando tabela...\")\n",
    "    df_full = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/{tablename}/\")\n",
    "    (df_full.coalesce(1).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema}.{tablename}\"))\n",
    "else:\n",
    "    print(\"Tabela já existente, ignorando a carga completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7490be8d-4a0a-4591-9bcb-f5b3d772bc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Atualização dos dados via Streaming + carga periódica do CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a943846-25bb-46f1-85ee-ea7978b403a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Carga CDC Do Kaggle"
    }
   },
   "outputs": [],
   "source": [
    "#def ingest_from_kaggle():\n",
    "kaggle_path = kagglehub.dataset_download(\"teocalvo/teomewhy-loyalty-system\")\n",
    "\n",
    "# Definir destino dos arquivos\n",
    "dest_path =  \"/Volumes/workspace/upsell/cdc/kaggle\"\n",
    "os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Copiando arquivos para o diretório de destino\n",
    "for file in os.listdir(kaggle_path):\n",
    "    if file.startswith(\"clientes\") and file.endswith(\".csv\"):\n",
    "        base, ext = os.path.splitext(file)\n",
    "        new_name = f\"{base}_{int(time.time())}{ext}\"\n",
    "        full_path = os.path.join(kaggle_path, file)\n",
    "\n",
    "        # Carregando snapshot completo do arquivo\n",
    "        df_kaggle = (spark.read.format(\"csv\")\n",
    "            .option(\"sep\", \";\")\n",
    "            .option(\"header\", True)\n",
    "            .load(full_path)\n",
    "        )\n",
    "\n",
    "        # Delimitação de período\n",
    "        periodo_carga = datetime.now() - timedelta(days=5)\n",
    "        df_filtrado = df_kaggle.filter(col(\"DtAtualizacao\") >= periodo_carga.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        print(\"Registros Filtrados: \", df_filtrado.count())\n",
    "\n",
    "        (df_filtrado.write\n",
    "            .option(\"sep\", \";\")\n",
    "            .option(\"header\", True)\n",
    "            .mode(\"overwrite\")\n",
    "            .csv(os.path.join(dest_path, new_name))\n",
    "        )\n",
    "\n",
    "        #shutil.copy(\n",
    "        #    os.path.join(kaggle_path, file), \n",
    "        #    os.path.join(dest_path, new_name)\n",
    "        #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981b45ef-4d92-4b28-96a3-c49145cdfadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{schema}.{tablename}\")\n",
    "\n",
    "def upsert(df, deltatable):\n",
    "  df.createOrReplaceGlobalTempView(f\"view_{tablename}\")\n",
    "\n",
    "  query = f'''\n",
    "      SELECT * \n",
    "      FROM global_temp.view_{tablename}\n",
    "      QUALIFY ROW_NUMBER() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "  '''\n",
    "\n",
    "  df_cdc = spark.sql(query)\n",
    "\n",
    "  (deltatable.alias(\"b\")\n",
    "             .merge(df_cdc.alias(\"d\"), f\"b.{id_field} = d.{id_field}\")\n",
    "             .whenMatchedUpdateAll()\n",
    "             .whenNotMatchedInsertAll()\n",
    "             .execute()\n",
    "  )\n",
    "\n",
    "  # Dataframe que realiza a leitura dos dados no formato stream.\n",
    "  df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"sep\", \";\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(f\"/Volumes/workspace/upsell/cdc/kaggle/{tablename}/\"))\n",
    "\n",
    "  # Etapa que realiza a persistência dos dados.\n",
    "  # Para cada batch recebido, ele aplicará uma função chamada upsert que recebe o pedaço de dados e a base onde será salvo.\n",
    "  stream = (df_stream.writeStream\n",
    "            .option(\"checkpointLocation\", f\"/Volumes/workspace/upsell/cdc/{tablename}_checkpoint/\")\n",
    "            .foreachBatch(lambda df, batchID: upsert(df, bronze))\n",
    "            #.trigger(processingTime=\"1 minute\")\n",
    "            .trigger(availableNow=True)\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b016a5c-6557-4e8c-9206-1e12d5063875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9d5970-8159-4203-a3ad-2a5d81c228d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lê o CSV do Kaggle diretamente (sem streaming)\n",
    "df_kaggle = (spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \";\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"/Volumes/workspace/upsell/cdc/kaggle/clientes.csv\"))\n",
    "\n",
    "print(\"Registros Kaggle:\", df_kaggle.count())\n",
    "\n",
    "# Carrega a tabela Bronze\n",
    "bronze = DeltaTable.forName(spark, \"bronze.customers\")\n",
    "\n",
    "# Conta antes\n",
    "before = bronze.toDF().count()\n",
    "print(\"Antes do merge:\", before)\n",
    "\n",
    "# Executa o merge manual\n",
    "(bronze.alias(\"b\")\n",
    "    .merge(df_kaggle.alias(\"d\"), \"b.idCliente = d.idCliente\")\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute())\n",
    "\n",
    "# Conta depois\n",
    "after = bronze.toDF().count()\n",
    "print(\"Depois do merge:\", after)\n",
    "print(\"Inseridos:\", after - before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4450f4-c012-48e3-a387-84f34a20f859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ler os arquivos no formato Streaming"
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{schema}.{tablename}\")\n",
    "\n",
    "def upsert(df, deltatable):\n",
    "  df.createOrReplaceGlobalTempView(f\"view_{tablename}\")\n",
    "\n",
    "  query = f'''\n",
    "      SELECT * \n",
    "      FROM global_temp.view_{tablename}\n",
    "      QUALIFY ROW_NUMBER() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "  '''\n",
    "\n",
    "  df_cdc = spark.sql(query)\n",
    "\n",
    "  (deltatable.alias(\"b\")\n",
    "             .merge(df_cdc.alias(\"d\"), f\"b.{id_field} = d.{id_field}\")\n",
    "             .whenMatchedDelete(condition = \"d.OP = 'D'\")\n",
    "             .whenMatchedUpdateAll(condition = \"d.OP = 'U'\")\n",
    "             .whenNotMatchedInsertAll(condition = \"d.OP = 'I' or d.OP = 'U'\")\n",
    "             .execute()\n",
    "  )\n",
    "\n",
    "# Dataframe que realiza a leitura dos dados no formato stream.\n",
    "df_stream = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\n",
    "  #.option(\"cloudFiles.maxFilesPerTrigger\", 500)\n",
    "  .schema(schema)\n",
    "  .load(f\"/Volumes/workspace/upsell/cdc/{tablename}/\"))\n",
    "\n",
    "# Etapa que realiza a persistência dos dados.\n",
    "# Para cada batch recebido, ele aplicará uma função chamada upsert que recebe o pedaço de dados e a base onde será salvo.\n",
    "stream = (df_stream.writeStream\n",
    "          .option(\"checkpointLocation\", f\"/Volumes/workspace/upsell/cdc/{tablename}_checkpoint/\")\n",
    "          .foreachBatch(lambda df, batchID: upsert(df, bronze))\n",
    "          .trigger(availableNow=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c83c4ae-151d-4a85-b20e-7c6c90c8ab3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6a5766-6c09-463e-80d6-e10d9167984a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cdc = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/cdc/kaggle\")\n",
    "ids_bronze = spark.table(f\"{database}.{tablename}\").select(id_field).distinct()\n",
    "ids_kaggle = df_cdc.select(id_field).distinct()\n",
    "ids_novos = ids_kaggle.subtract(ids_bronze)\n",
    "\n",
    "print(\"Novos IDs:\", ids_novos.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca8e959-ec22-421a-b8d9-b9d85ac9cc7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM bronze.customers"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7104747547924429,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
