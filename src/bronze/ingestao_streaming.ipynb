{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f82a92-9840-438d-9166-4e9bee9c7299",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports biblioteca"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "\n",
    "def table_exists(database, table):\n",
    "    count = (spark.sql(f\"SHOW TABLES FROM {database}\")\n",
    "                  .filter(f\"database='{database}' AND tableName='{table}'\")\n",
    "                  .count())    \n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1d4814-2010-401b-ad51-285118094bd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup: Parametrização"
    }
   },
   "outputs": [],
   "source": [
    "schema = \"bronze\"\n",
    "tablename = \"customers\"\n",
    "id_field = \"idCliente\"\n",
    "timestamp_field = \"DtAtualizacao\"\n",
    "\n",
    "# tablename = dbutils.widgets.get(\"tablename\")\n",
    "# id_field = dbutils.widgets.get(\"id_field\")\n",
    "# timestamp_field = dbutils.widgets.get(\"timestamp_field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48f3a4-cc29-489a-bf0f-f0463b07f6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O arquivo CSV não possui schea, então foi passado alguns parâmetros para ele definir. \n",
    "# Já com arquivos Parquet,  naturalmente já vem o schema inferido pois possui metadados.\n",
    "df_full = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/{tablename}/\")\n",
    "schema = df_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5163371-3805-44e8-9245-ce253dd64d47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão Carga Completa"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(schema, tablename):\n",
    "    print(\"Tabela não existente, criando tabela...\")\n",
    "    df_full = spark.read.format(\"csv\").options(sep=\";\", header=True).load(f\"/Volumes/workspace/upsell/full_load/{tablename}/\")\n",
    "    (df_full.coalesce(1).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema}.{tablename}\"))\n",
    "else:\n",
    "    print(\"Tabela já existente, ignorando a carga completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7490be8d-4a0a-4591-9bcb-f5b3d772bc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Atualização da tabela - ReadStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4450f4-c012-48e3-a387-84f34a20f859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ler os arquivos no formato Streaming"
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{schema}.{tablename}\")\n",
    "\n",
    "# Dataframe que realiza a leitura dos dados no formato stream.\n",
    "df_stream = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\n",
    "  .schema(schema)\n",
    "  .load(f\"/Volumes/workspace/upsell/cdc/{tablename}/\"))\n",
    "\n",
    "stream = (df_stream.writeStream\n",
    "          .option(\"checkpointLocation\", f\"/Volumes/workspace/upsell/cdc/{tablename}_checkpoint/\")\n",
    "          .foreachBatch()\n",
    "        )\n",
    "\n",
    "\n",
    "def upsert(df, deltatable):\n",
    "  query = f'''\n",
    "      SELECT * \n",
    "      FROM view_{tablename}\n",
    "      QUALIFY ROW_NUMBER() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "  '''\n",
    "\n",
    "  df_cdc = spark.sql(query)\n",
    "\n",
    "  (deltatable.alias(\"b\")\n",
    "        .merge(df_cdc.alias(\"d\"), f\"b.{id_field} = d.{id_field}\")\n",
    "        .whenMatchedDelete(condition = \"d.OP = 'D'\")\n",
    "        .whenMatchedUpdateAll(condition = \"d.OP = 'U'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.OP = 'I' or d.OP = 'U'\")\n",
    "        #.execute()\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5042236570075985,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
